{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"\u4e2d\u6587 | \ud55c\uad6d\uc5b4 | \u65e5\u672c\u8a9e | \u0420\u0443\u0441\u0441\u043a\u0438\u0439 | Deutsch | Fran\u00e7ais | Espa\u00f1ol | Portugu\u00eas | T\u00fcrk\u00e7e | Ti\u1ebfng Vi\u1ec7t | \u0627\u0644\u0639\u0631\u0628\u064a\u0629 <p>Introducing Ultralytics YOLO11, the latest version of the acclaimed real-time object detection and image segmentation model. YOLO11 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.</p> <p>Explore the Ultralytics Docs, a comprehensive resource designed to help you understand and utilize its features and capabilities. Whether you are a seasoned machine learning practitioner or new to the field, this hub aims to maximize YOLO's potential in your projects</p>"},{"location":"#where-to-start","title":"Where to Start","text":"- :material-clock-fast:{ .lg .middle } \u00a0 **Getting Started**      ***      Install `ultralytics` with pip and get up and running in minutes to train a YOLO model      ***      [:octicons-arrow-right-24: Quickstart](quickstart.md)  - :material-image:{ .lg .middle } \u00a0 **Predict**      ***      Predict on new images, videos and streams with YOLO  \u00a0      ***      [:octicons-arrow-right-24: Learn more](modes/predict.md)  - :fontawesome-solid-brain:{ .lg .middle } \u00a0 **Train a Model**      ***      Train a new YOLO model on your own custom dataset from scratch or load and train on a pretrained model      ***      [:octicons-arrow-right-24: Learn more](modes/train.md)  - :material-magnify-expand:{ .lg .middle } \u00a0 **Explore Tasks**      ***      Discover YOLO tasks like detect, segment, classify, pose, OBB and track  \u00a0      ***      [:octicons-arrow-right-24: Explore Tasks](tasks/index.md)  - :rocket:{ .lg .middle } \u00a0 **Explore YOLO11 NEW**      ***      Discover Ultralytics' latest state-of-the-art YOLO11 models and their capabilities  \u00a0      ***      [:octicons-arrow-right-24: YOLO11 Models \ud83d\ude80 NEW](models/yolo11.md)  - :material-scale-balance:{ .lg .middle } \u00a0 **Open Source, AGPL-3.0**      ***      Ultralytics offers two licensing options for YOLO: AGPL-3.0 License and Enterprise License. Ultralytics is available on [GitHub](https://github.com/ultralytics/ultralytics)      ***      [:octicons-arrow-right-24: License](https://www.ultralytics.com/license)   <p> Watch: How to Train a YOLO model on Your Custom Dataset in Google Colab. </p>"},{"location":"#yolo-a-brief-history","title":"YOLO: A Brief History","text":"<p>YOLO (You Only Look Once), a popular object detection and image segmentation model, was developed by Joseph Redmon and Ali Farhadi at the University of Washington. Launched in 2015, YOLO quickly gained popularity for its high speed and accuracy.</p> <ul> <li>YOLOv2, released in 2016, improved the original model by incorporating batch normalization, anchor boxes, and dimension clusters.</li> <li>YOLOv3, launched in 2018, further enhanced the model's performance using a more efficient backbone network, multiple anchors and spatial pyramid pooling.</li> <li>YOLOv4 was released in 2020, introducing innovations like Mosaic data augmentation, a new anchor-free detection head, and a new loss function.</li> <li>YOLOv5 further improved the model's performance and added new features such as hyperparameter optimization, integrated experiment tracking and automatic export to popular export formats.</li> <li>YOLOv6 was open-sourced by Meituan in 2022 and is in use in many of the company's autonomous delivery robots.</li> <li>YOLOv7 added additional tasks such as pose estimation on the COCO keypoints dataset.</li> <li>YOLOv8 released in 2023 by Ultralytics. YOLOv8 introduced new features and improvements for enhanced performance, flexibility, and efficiency, supporting a full range of vision AI tasks,</li> <li>YOLOv9 introduces innovative methods like Programmable Gradient Information (PGI) and the Generalized Efficient Layer Aggregation Network (GELAN).</li> <li>YOLOv10 is created by researchers from Tsinghua University using the Ultralytics Python package. This version provides real-time object detection advancements by introducing an End-to-End head that eliminates Non-Maximum Suppression (NMS) requirements.</li> <li>YOLO11 \ud83d\ude80 NEW: Ultralytics' latest YOLO models delivering state-of-the-art (SOTA) performance across multiple tasks, including detection, segmentation, pose estimation, tracking, and classification, leverage capabilities across diverse AI applications and domains.</li> </ul>"},{"location":"#yolo-licenses-how-is-ultralytics-yolo-licensed","title":"YOLO Licenses: How is Ultralytics YOLO licensed?","text":"<p>Ultralytics offers two licensing options to accommodate diverse use cases:</p> <ul> <li>AGPL-3.0 License: This OSI-approved open-source license is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See the LICENSE file for more details.</li> <li>Enterprise License: Designed for commercial use, this license permits seamless integration of Ultralytics software and AI models into commercial goods and services, bypassing the open-source requirements of AGPL-3.0. If your scenario involves embedding our solutions into a commercial offering, reach out through Ultralytics Licensing.</li> </ul> <p>Our licensing strategy is designed to ensure that any improvements to our open-source projects are returned to the community. We hold the principles of open source close to our hearts \u2764\ufe0f, and our mission is to guarantee that our contributions can be utilized and expanded upon in ways that are beneficial to all.</p>"},{"location":"#faq","title":"FAQ","text":""},{"location":"#what-is-ultralytics-yolo-and-how-does-it-improve-object-detection","title":"What is Ultralytics YOLO and how does it improve object detection?","text":"<p>Ultralytics YOLO is the latest advancement in the acclaimed YOLO (You Only Look Once) series for real-time object detection and image segmentation. It builds on previous versions by introducing new features and improvements for enhanced performance, flexibility, and efficiency. YOLO supports various vision AI tasks such as detection, segmentation, pose estimation, tracking, and classification. Its state-of-the-art architecture ensures superior speed and accuracy, making it suitable for diverse applications, including edge devices and cloud APIs.</p>"},{"location":"#how-can-i-get-started-with-yolo-installation-and-setup","title":"How can I get started with YOLO installation and setup?","text":"<p>Getting started with YOLO is quick and straightforward. You can install the Ultralytics package using pip and get up and running in minutes. Here's a basic installation command:</p> <p>!!! example \"Installation using pip\"</p> <pre><code>=== \"CLI\"\n\n    ```bash\n    pip install ultralytics\n    ```\n</code></pre> <p>For a comprehensive step-by-step guide, visit our quickstart guide. This resource will help you with installation instructions, initial setup, and running your first model.</p>"},{"location":"#how-can-i-train-a-custom-yolo-model-on-my-dataset","title":"How can I train a custom YOLO model on my dataset?","text":"<p>Training a custom YOLO model on your dataset involves a few detailed steps:</p> <ol> <li>Prepare your annotated dataset.</li> <li>Configure the training parameters in a YAML file.</li> <li>Use the <code>yolo TASK train</code> command to start training. (Each <code>TASK</code> has its own argument)</li> </ol> <p>Here's example code for the Object Detection Task:</p> <p>!!! example \"Train Example for Object Detection Task\"</p> <pre><code>=== \"Python\"\n\n    ```python\n    from ultralytics import YOLO\n\n    # Load a pre-trained YOLO model (you can choose n, s, m, l, or x versions)\n    model = YOLO(\"yolo11n.pt\")\n\n    # Start training on your custom dataset\n    model.train(data=\"path/to/dataset.yaml\", epochs=100, imgsz=640)\n    ```\n\n=== \"CLI\"\n\n    ```bash\n    # Train a YOLO model from the command line\n    yolo detect train data=path/to/dataset.yaml epochs=100 imgsz=640\n    ```\n</code></pre> <p>For a detailed walkthrough, check out our Train a Model guide, which includes examples and tips for optimizing your training process.</p>"},{"location":"#what-are-the-licensing-options-available-for-ultralytics-yolo","title":"What are the licensing options available for Ultralytics YOLO?","text":"<p>Ultralytics offers two licensing options for YOLO:</p> <ul> <li>AGPL-3.0 License: This open-source license is ideal for educational and non-commercial use, promoting open collaboration.</li> <li>Enterprise License: This is designed for commercial applications, allowing seamless integration of Ultralytics software into commercial products without the restrictions of the AGPL-3.0 license.</li> </ul> <p>For more details, visit our Licensing page.</p>"},{"location":"#how-can-ultralytics-yolo-be-used-for-real-time-object-tracking","title":"How can Ultralytics YOLO be used for real-time object tracking?","text":"<p>Ultralytics YOLO supports efficient and customizable multi-object tracking. To utilize tracking capabilities, you can use the <code>yolo track</code> command as shown below:</p> <p>!!! example \"Example for Object Tracking on a Video\"</p> <pre><code>=== \"Python\"\n\n    ```python\n    from ultralytics import YOLO\n\n    # Load a pre-trained YOLO model\n    model = YOLO(\"yolo11n.pt\")\n\n    # Start tracking objects in a video\n    # You can also use live video streams or webcam input\n    model.track(source=\"path/to/video.mp4\")\n    ```\n\n=== \"CLI\"\n\n    ```bash\n    # Perform object tracking on a video from the command line\n    # You can specify different sources like webcam (0) or RTSP streams\n    yolo track source=path/to/video.mp4\n    ```\n</code></pre> <p>For a detailed guide on setting up and running object tracking, check our tracking mode documentation, which explains the configuration and practical applications in real-time scenarios.</p>"},{"location":"tofu_counting/","title":"Fried-Tofu-Counting","text":"<p>Prepared by: Manan Patel</p>"},{"location":"tofu_counting/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Estimation &amp; Timeline</li> <li>Approaches</li> <li>Hardware Specifications / Tech Stack</li> <li>Dataset Information</li> </ol>"},{"location":"tofu_counting/#hardware-specifications-tech-stack","title":"Hardware Specifications / Tech Stack","text":"<ul> <li>System: Jetson Orien Nano</li> <li>Performance: ~23 FPS</li> </ul>"},{"location":"tofu_counting/approaches/","title":"Approaches","text":""},{"location":"tofu_counting/approaches/#frame-processing","title":"Frame Processing","text":"<p>Iterates through each frame of the input video.</p>"},{"location":"tofu_counting/approaches/#window-detection","title":"Window Detection","text":"<p>Uses the YOLOv8 Nano window detection model to detect windows containing tofus: - Classes: Full, Empty, Half.</p>"},{"location":"tofu_counting/approaches/#tofu-detection","title":"Tofu Detection","text":"<ul> <li>Triggered only when the window detection model gives a \"Full\" class.</li> <li>Cropped window images are fed into the tofu detection model.</li> <li>Models tried:</li> <li>YOLOv8 Segmentation (Nano, Small, Medium)</li> <li>FastSAM Small</li> <li>Final Choice: YOLOv8 Nano Detection, which gave satisfactory results.</li> </ul>"},{"location":"tofu_counting/approaches/#annotation-and-output","title":"Annotation and Output","text":"<ul> <li>Annotates frames with detected tofus.</li> <li>Saves results in:</li> <li>Annotated output video.</li> <li>CSV file with detection timings and counts.</li> </ul>"},{"location":"tofu_counting/approaches/#counting-mechanism","title":"Counting Mechanism","text":"<ul> <li>Counts tofus based on detected regions.</li> <li>Maintains a cumulative count throughout the video.</li> </ul>"},{"location":"tofu_counting/approaches/#performance-metrics","title":"Performance Metrics","text":"<ul> <li>Logs performance metrics:</li> <li>Frame processing time.</li> <li>Model inference time.</li> <li>Overall script execution time (displayed in the terminal).</li> </ul>"},{"location":"tofu_counting/dataset_info/","title":"Dataset Information","text":""},{"location":"tofu_counting/dataset_info/#window-detection-model","title":"Window Detection Model","text":"<ul> <li>Total Images: 705 annotated images.</li> <li>Classes: Full, Empty, Half.</li> </ul>"},{"location":"tofu_counting/dataset_info/#tofu-detection-model","title":"Tofu Detection Model","text":"<ul> <li>Total Images: 306 cropped window images.</li> <li>Annotations: Annotated tofus within windows.</li> </ul>"},{"location":"tofu_counting/estimation/","title":"Estimation &amp; Timeline","text":"Task ETA R&amp;D for Approaches 4 Hours Window Detection Model (Annotations and Training) 4 Hours Tofu Detection Model (Annotations and Training) 10 Hours Logic Writing and Inference using Trained Models 10 Hours Setup and Inference on Jetson Orien Nano 2 Hours Documentation for the Project 4 Hours Total 34 Hours"},{"location":"tofu_counting/introduction/","title":"Introduction","text":"<p>As an input, a video of a system in which tofus are passing through a potential area (window) is provided. The objective is: 1. To count the number of tofus each time a bunch of tofus passes through the window. 2. To count the total number of tofus in the entire video. 3. Display the output in the video and create a CSV file that includes the count of tofu in a bunch from its first occurrence.</p>"},{"location":"tofu_counting/introduction/#useful-links","title":"Useful Links:","text":"<ul> <li>The dataset shared by the client: Dataset Video by Client</li> <li>Jira Epic: Fried Tofu Counting Jira Issue</li> <li>Project Zip Link: Fried Tofu Counting Zip</li> <li>NAS Paths:</li> <li>Window Detection Model: <code>/media/ai_projects/Fried Tofu Counting/Window Detection Models</code></li> <li>Tofu Detection Model: <code>/media/ai_projects/Fried Tofu Counting/Tofu Detection Models</code></li> </ul>"}]}